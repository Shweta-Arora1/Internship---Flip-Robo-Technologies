{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the most viewed videos\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "\n",
    "# Initialize empty lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Extract the details from each row in the table\n",
    "rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip().replace(\",\", \"\")\n",
    "\n",
    "    # Append the details to the respective lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print the details of the most viewed videos\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}\")\n",
    "    print(f\"Name: {name_list[i]}\")\n",
    "    print(f\"Artist: {artist_list[i]}\")\n",
    "    print(f\"Upload Date: {upload_date_list[i]}\")\n",
    "    print(f\"Views: {views_list[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3258c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the BCCI.tv homepage\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the international fixtures link on the homepage\n",
    "fixtures_link = soup.find(\"a\", text=\"International Fixtures\")[\"href\"]\n",
    "\n",
    "# Construct the URL for the international fixtures page\n",
    "fixtures_url = f\"{url.rstrip('/')}{fixtures_link}\"\n",
    "\n",
    "# Send a GET request to the international fixtures page\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "# Create another BeautifulSoup object for the fixtures page\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.content, \"html.parser\")\n",
    "\n",
    "# Find the container for the fixtures\n",
    "fixtures_container = fixtures_soup.find(\"div\", {\"class\": \"js-list\"})\n",
    "\n",
    "# Find all the fixtures in the container\n",
    "fixtures = fixtures_container.find_all(\"li\")\n",
    "\n",
    "# Iterate through each fixture and extract the details\n",
    "for fixture in fixtures:\n",
    "    match_title = fixture.find(\"p\", {\"class\": \"fixture__additional-info\"}).text.strip()\n",
    "    series = fixture.find(\"span\", {\"class\": \"u-unskewed-text\"}).text.strip()\n",
    "    place = fixture.find(\"p\", {\"class\": \"fixture__additional-info\"}).find_next(\"span\").text.strip()\n",
    "    date = fixture.find(\"div\", {\"class\": \"fixture__full-date\"}).text.strip()\n",
    "    time = fixture.find(\"div\", {\"class\": \"fixture__time\"}).text.strip()\n",
    "\n",
    "    # Print the details of the fixture\n",
    "    print(f\"Match Title: {match_title}\")\n",
    "    print(f\"Series: {series}\")\n",
    "    print(f\"Place: {place}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Time: {time}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54661a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the highest-selling novels\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Initialize empty lists to store the details\n",
    "book_list = []\n",
    "author_list = []\n",
    "volumes_sold_list = []\n",
    "publisher_list = []\n",
    "genre_list = []\n",
    "\n",
    "# Extract the details from each row in the table\n",
    "rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    # Append the details to the respective lists\n",
    "    book_list.append(book_name)\n",
    "    author_list.append(author_name)\n",
    "    volumes_sold_list.append(volumes_sold)\n",
    "    publisher_list.append(publisher)\n",
    "    genre_list.append(genre)\n",
    "\n",
    "# Print the details of the highest-selling novels\n",
    "for i in range(len(book_list)):\n",
    "    print(f\"Book Name: {book_list[i]}\")\n",
    "    print(f\"Author Name: {author_list[i]}\")\n",
    "    print(f\"Volumes Sold: {volumes_sold_list[i]}\")\n",
    "    print(f\"Publisher: {publisher_list[i]}\")\n",
    "    print(f\"Genre: {genre_list[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container for the TV series\n",
    "series_container = soup.find(\"div\", class_=\"lister-list\")\n",
    "\n",
    "# Find all the TV series in the container\n",
    "series = series_container.find_all(\"div\", class_=\"lister-item mode-detail\")\n",
    "\n",
    "# Iterate through each TV series and extract the details\n",
    "for serie in series:\n",
    "    # Extract the name\n",
    "    name = serie.find(\"h3\").find(\"a\").text.strip()\n",
    "\n",
    "    # Extract the year span\n",
    "    year_span = serie.find(\"span\", class_=\"lister-item-year\").text.strip()\n",
    "\n",
    "    # Extract the genre\n",
    "    genre = serie.find(\"span\", class_=\"genre\").text.strip()\n",
    "\n",
    "    # Extract the run time\n",
    "    run_time = serie.find(\"span\", class_=\"runtime\").text.strip()\n",
    "\n",
    "    # Extract the ratings\n",
    "    ratings = serie.find(\"div\", class_=\"ipl-rating-star small\").find(\"span\", class_=\"ipl-rating-star__rating\").text.strip()\n",
    "\n",
    "    # Extract the votes\n",
    "    votes = serie.find(\"p\", class_=\"text-muted text-small\").find(\"span\", attrs={\"name\": \"nv\"}).text.strip()\n",
    "\n",
    "    # Print the details of the TV series\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Year Span: {year_span}\")\n",
    "    print(f\"Genre: {genre}\")\n",
    "    print(f\"Run Time: {run_time}\")\n",
    "    print(f\"Ratings: {ratings}\")\n",
    "    print(f\"Votes: {votes}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7baf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"Show All Dataset\" page\n",
    "show_all_link = soup.find(\"a\", href=\"ml/datasets.php\")\n",
    "\n",
    "# Get the URL of the \"Show All Dataset\" page\n",
    "show_all_url = url + show_all_link[\"href\"]\n",
    "\n",
    "# Send a GET request to the \"Show All Dataset\" page\n",
    "show_all_response = requests.get(show_all_url)\n",
    "\n",
    "# Create another BeautifulSoup object to parse the \"Show All Dataset\" page\n",
    "show_all_soup = BeautifulSoup(show_all_response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the dataset details\n",
    "table = show_all_soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "# Iterate through each row and extract the details\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    # Extract the dataset name\n",
    "    dataset_name = columns[0].text.strip()\n",
    "\n",
    "    # Extract the data type\n",
    "    data_type = columns[1].text.strip()\n",
    "\n",
    "    # Extract the task\n",
    "    task = columns[2].text.strip()\n",
    "\n",
    "    # Extract the attribute type\n",
    "    attribute_type = columns[3].text.strip()\n",
    "\n",
    "    # Extract the number of instances\n",
    "    num_instances = columns[4].text.strip()\n",
    "\n",
    "    # Extract the number of attributes\n",
    "    num_attributes = columns[5].text.strip()\n",
    "\n",
    "    # Extract the year\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    # Print the details of the dataset\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Attribute Type: {attribute_type}\")\n",
    "    print(f\"Number of Instances: {num_instances}\")\n",
    "    print(f\"Number of Attributes: {num_attributes}\")\n",
    "    print(f\"Year: {year}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9 Answer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the recruiters page\n",
    "url = \"https://www.naukri.com/hr-recruiters-consultants\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the recruiters option link on the homepage\n",
    "recruiters_link = soup.find(\"a\", text=\"Recruiters\")\n",
    "\n",
    "# Get the URL of the recruiters page\n",
    "recruiters_url = recruiters_link[\"href\"]\n",
    "\n",
    "# Send a GET request to the recruiters page\n",
    "recruiters_response = requests.get(recruiters_url)\n",
    "\n",
    "# Create another BeautifulSoup object to parse the recruiters page\n",
    "recruiters_soup = BeautifulSoup(recruiters_response.content, \"html.parser\")\n",
    "\n",
    "# Find the search input field\n",
    "search_input = recruiters_soup.find(\"input\", id=\"root_search\")\n",
    "\n",
    "# Set the value of the search input field to \"Data science\"\n",
    "search_input[\"value\"] = \"Data science\"\n",
    "\n",
    "# Submit the search form\n",
    "recruiters_form = search_input.find_parent(\"form\")\n",
    "recruiters_form.submit()\n",
    "\n",
    "# Create another BeautifulSoup object to parse the search results page\n",
    "search_results_soup = BeautifulSoup(recruiters_response.content, \"html.parser\")\n",
    "\n",
    "# Find the container for the search results\n",
    "results_container = search_results_soup.find(\"section\", class_=\"content\")\n",
    "\n",
    "# Find all the recruiter profiles in the container\n",
    "recruiter_profiles = results_container.find_all(\"article\")\n",
    "\n",
    "# Iterate through each recruiter profile and extract the details\n",
    "for profile in recruiter_profiles:\n",
    "    # Extract the name\n",
    "    name = profile.find(\"a\", class_=\"rec_name\").text.strip()\n",
    "\n",
    "    # Extract the designation\n",
    "    designation = profile.find(\"div\", class_=\"rec_designation\").text.strip()\n",
    "\n",
    "    # Extract the company\n",
    "    company = profile.find(\"div\", class_=\"rec_company\").text.strip()\n",
    "\n",
    "    # Extract the skills they hire for\n",
    "    skills = profile.find(\"div\", class_=\"rec_skills\").text.strip()\n",
    "\n",
    "    # Extract the location\n",
    "    location = profile.find(\"div\", class_=\"rec_location\").text.strip()\n",
    "\n",
    "    # Print the details of the recruiter\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Designation: {designation}\")\n",
    "    print(f\"Company: {company}\")\n",
    "    print(f\"Skills They Hire For: {skills}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
